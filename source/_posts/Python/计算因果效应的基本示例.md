---
title: 计算因果效应的基本示例
date: 2025-04-29 00:07:05
tags: [因果推断]
categories: [[Python, 因果推断]]
---


<!-- reticulate::use_python("C:/ProgramData/Anaconda3/python.exe") -->
<!-- https://www.pywhy.org/dowhy/v0.12/example_notebooks/dowhy_simple_example.html -->

这是一个关于 DoWhy
因果推断库的快速入门介绍。这里，我们将加载一个示例数据集，并估计一个（预先指定的）处理变量对一个（预先指定的）结果变量的因果效应。

# 基础概念

在开始代码前，我们需要了解几个关键概念：

<ul style="margin-top:0;">
<li style="margin-top:2px;margin-bottom:2px;">
<strong>因果效应</strong>：比如”吃药(v0)对康复(y)的影响”，而不是简单的相关性
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>治疗变量 Treatment</strong>：我们关注的干预因素，如是否吃药
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>结果变量 Outcome</strong>：我们关心的结果，如健康程度
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>混杂因素
Confounder</strong>：既影响治疗又影响结果的变量，如病情严重程度
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>工具变量 Instrumental
Variable</strong>：只影响治疗但不直接影响结果的变量
</li>
</ul>

# 导入所需库

``` python
import numpy as np
import dowhy.datasets
from dowhy import CausalModel
```

# 构造模拟数据

现在，让我们加载一个数据集。为了简单起见，我们模拟一个数据集，其中共同原因与处理变量之间、共同原因与结果变量之间都是线性关系。

`beta`是真实的因果效应。

``` python
data = dowhy.datasets.linear_dataset(
  beta = 10,                      # 真实的因果效应大小
  num_common_causes = 5,          # 5个混杂因素
  num_instruments = 2,            # 2个工具变量  
  num_effect_modifiers = 1 ,      # 1个效应修饰变量
  num_samples = 5000,             # 5000个样本
  treatment_is_binary = True,     # 治疗是二元的(是/否)
  stddev_treatment_noise = 10,    # 治疗变量的噪声大小
  num_discrete_common_causes = 1  # 1个离散型混杂因素
)

df = data["df"]                   # 获取生成的数据框
df.head()                         # 查看前几行数据
##          X0   Z0        Z1        W0  ...        W3  W4     v0          y
## 0 -0.056755  0.0  0.236210  0.428525  ... -3.136011   0  False -16.989686
## 1 -1.279875  0.0  0.351631 -1.106842  ... -0.911872   2  False  -4.094126
## 2  1.637977  0.0  0.225216 -0.576326  ... -0.784004   0  False  -2.876001
## 3  0.920122  0.0  0.894980  1.385943  ... -3.609170   2  False -16.598985
## 4  0.400167  0.0  0.307448 -0.024350  ...  0.391856   1  False  -7.492589
##                                                                          
##                           [5 rows x 10 columns]
```

<strong style="color:#00A087;font-size:16px;">这里创建了一个模拟数据集</strong>：

<ul style="margin-top:0;">
<li style="margin-top:2px;margin-bottom:2px;">
<strong>beta = 10</strong>：设定真实的因果效应是
10，比如吃药平均能让健康评分提高 10 分
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>num_common_causes = 5</strong>：有 5
个共同影响因素，既影响治疗也影响结果
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>num_instruments = 2</strong>：有 2
个工具变量，只影响治疗不影响结果
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>num_effect_modifiers = 1</strong>：生成 1
个效应修饰变量，这个变量会改变治疗的效果大小
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>num_samples = 5000</strong>：5000 个样本
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>treatment_is_binary = True</strong>：治疗变量是二元的
</li>
</ul>

<strong style="color:#E50914;font-size:16px;">生成的数据结构</strong>：

<ul style="margin-top:0;">
<li style="margin-top:2px;margin-bottom:2px;">
<strong>Z0, Z1</strong>：工具变量
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>W0-W4</strong>：混杂变量
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>v0</strong>：治疗变量，是否吃药
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>y</strong>：结果变量，健康评分
</li>
<li style="margin-top:2px;margin-bottom:2px;">
<strong>X0</strong>：效应修饰变量
</li>
</ul>
<p>
生成一个线性结构的数据集，模拟了真实的因果关系。beta = 10
代表真实的因果效应（treatment 对 outcome 的真实平均影响是10）。
</p>
<p>
数据有 5 个混杂变量（common causes），2个工具变量（instruments），1
个效应修饰变量（effect modifier），样本量
5000，处理变量是二元的（0/1），treatment 有噪声。
</p>
<p>
请注意，我们需要使用 pandas DataFrame 来加载数据，因为目前，DoWhy
只支持以 pandas DataFrame 作为输入。
</p>

# 构建因果模型

第一种（Without graph），没有因果图，必须手动指定共同原因（common
causes）、工具变量（instruments）、效应修饰变量（effect
modifiers）等；第二种（With graph），你直接输入了一个因果图（causal
graph），模型会自动根据图结构推断哪些是共同原因、工具变量等。

## 指定变量列表（无因果图）

``` python
# 没有传入因果图，而是直接传入变量名，模型不会帮你自动识别
# 适用于，变量不多，自己可以手动列出哪些是共同原因，哪些是修饰变量
model= CausalModel(
  data = df,
  treatment = data["treatment_name"],
  outcome = data["outcome_name"],
  common_causes = data["common_causes_names"],
  effect_modifiers = data["effect_modifier_names"]
)

model.view_model()
```

<img src="计算因果效应的基本示例_files/figure-markdown_strict/unnamed-chunk-3-1.png" width="768" />

## 使用因果图（因果图，推荐）

``` python
# 使用因果图，适合变量之间关系复杂，或者变量太多，不容易手动分类的场景
model = CausalModel(
  data = df,
  treatment = data["treatment_name"],  # 指定治疗变量名(这里是"v0")
  outcome = data["outcome_name"],      # 指定结果变量名(这里是"y") 
  graph = data["gml_graph"]            # 提供因果图
)

# 可视化因果图
model.view_model()
```

<img src="计算因果效应的基本示例_files/figure-markdown_strict/unnamed-chunk-4-3.png" width="768" />

`.view_model()`会生成因果图像文件，上因果图展示了因果模型中所包含的假设，因果图清晰地展示了变量间的因果关系：

-   Z0,Z1 → v0 (工具变量影响治疗)
-   W0-W4 → v0 和 W0-W4 → y (混杂因素影响治疗和结果)
-   v0 → y (治疗影响结果)
-   X0 → y (效应修饰变量影响结果)

这里使用了GML格式的因果图，这是一种图形描述语言。现在，我们可以利用该图来识别因果效应（即从因果估计量转化为概率表达式），然后再估计因果效应。

# 因果识别

`DoWhy`的理念，将识别 Identification 与估计 Estimation 分开。

识别阶段只需要访问因果图，而不需要接触数据。这个步骤会得到一个需要计算的表达式。之后，在估计阶段，可以利用数据来计算该表达式。

需要理解的是，这两个步骤是相互独立的。

``` python
identified_estimand = model.identify_effect(proceed_when_unidentifiable = True)

print(identified_estimand)
## Estimand type: EstimandType.NONPARAMETRIC_ATE
## 
## ### Estimand : 1
## Estimand name: backdoor
## Estimand expression:
##   d                       
## ─────(E[y|W3,W4,W2,W0,W1])
## d[v₀]                     
## Estimand assumption 1, Unconfoundedness: If U→{v0} and U→y then P(y|v0,W3,W4,W2,W0,W1,U) = P(y|v0,W3,W4,W2,W0,W1)
## 
## ### Estimand : 2
## Estimand name: iv
## Estimand expression:
##  ⎡                              -1⎤
##  ⎢    d        ⎛    d          ⎞  ⎥
## E⎢─────────(y)⋅⎜─────────([v₀])⎟  ⎥
##  ⎣d[Z₁  Z₀]    ⎝d[Z₁  Z₀]      ⎠  ⎦
## Estimand assumption 1, As-if-random: If U→→y then ¬(U →→{Z1,Z0})
## Estimand assumption 2, Exclusion: If we remove {Z1,Z0}→{v0}, then ¬({Z1,Z0}→y)
## 
## ### Estimand : 3
## Estimand name: frontdoor
## No such variable(s) found!
```

`proceed_when_unidentifiable = True`表示我们假设可以忽略任何未观测到的混杂因素。

默认情况下，系统会提示用户再次确认是否可以忽略未观测混杂因素。

这一步仅基于因果图结构和变量定义，推导出`应如何计算`因果效应，并给出表达式和假设。

只要控制住 W3, W4, W1, W0, W2 这些混杂变量，v0 对 y
的平均因果效应就是可识别的。

# 因果效应估计

``` python
causal_estimate = model.estimate_effect(
  identified_estimand,
  method_name = "backdoor.propensity_score_stratification"
)

print(causal_estimate)
## *** Causal Estimate ***
## 
## ## Identified estimand
## Estimand type: EstimandType.NONPARAMETRIC_ATE
## 
## ### Estimand : 1
## Estimand name: backdoor
## Estimand expression:
##   d                       
## ─────(E[y|W3,W4,W2,W0,W1])
## d[v₀]                     
## Estimand assumption 1, Unconfoundedness: If U→{v0} and U→y then P(y|v0,W3,W4,W2,W0,W1,U) = P(y|v0,W3,W4,W2,W0,W1)
## 
## ## Realized estimand
## b: y~v0+W3+W4+W2+W0+W1
## Target units: ate
## 
## ## Estimate
## Mean value: 11.262266744799229
```

估计的因果效应与设定的真实值 10
很接近，但由于数据中有噪声，估计值不是精确的 10。

# 不同群体的效应估计

``` python
estimate = model.estimate_effect(
  identified_estimand,
  method_name = "backdoor.propensity_score_stratification",
  target_units = "atc"
)

print(estimate)
## *** Causal Estimate ***
## 
## ## Identified estimand
## Estimand type: EstimandType.NONPARAMETRIC_ATE
## 
## ### Estimand : 1
## Estimand name: backdoor
## Estimand expression:
##   d                       
## ─────(E[y|W3,W4,W2,W0,W1])
## d[v₀]                     
## Estimand assumption 1, Unconfoundedness: If U→{v0} and U→y then P(y|v0,W3,W4,W2,W0,W1,U) = P(y|v0,W3,W4,W2,W0,W1)
## 
## ## Realized estimand
## b: y~v0+W3+W4+W2+W0+W1
## Target units: atc
## 
## ## Estimate
## Mean value: 11.250168750807648
print("Causal Estimate is " + str(estimate.value))
## Causal Estimate is 11.250168750807648
```

`ate`，全体样本的平均效应

`att`，治疗组的平均效应

`atc`，对照组的平均效应

# 模型验证

验证是确保我们的估计可靠的关键步骤。

验证方法为每一个正确的估计器提供了应通过的检验，如果某个估计器未能通过验证检验（即
p 值小于 0.05），这说明该估计器存在一些问题。

需要注意的是，我们无法验证估计值一定是正确的，但如果它违反了一些预期行为，我们可以拒绝它（类似于科学理论可以被证伪但无法被证明为真）。

下面的验证测试基于以下两类思路：

-   不变性变换（Invariant
    transformations）：对数据进行一些不会改变因果效应的变换。任何在原始数据和变换后数据之间结果有显著变化的估计器，都未通过该测试。

-   归零变换（Nullifying
    transformations）：数据经过改变后，真实的因果效应应为零。任何在新数据上结果显著偏离零的估计器，都未通过该测试。

为了保证验证实验结果的可复现性，你可以在任何验证方法中加入`random_seed`参数。

还可以通过内置的并行化机制加速验证过程，只需设置 n_jobs 参数大于
1，即可将任务分配到多个 CPU，或设置`n_jobs = -1`使用所有可用
CPU。这样可以大大加快大数据量或需要多次重复实验时的计算速度。

## 添加随机共同原因

添加一个随机生成的混杂变量，如果估计结果变化很大，说明原估计不可靠。

``` python
res_random = model.refute_estimate(
  identified_estimand,
  estimate, 
  method_name = "random_common_cause",
  random_seed = 1,
  n_jobs = 1,
  show_progress_bar = True
)
## Refuting Estimates:   0%|[32m          [0m| 0/100 [00:00<?, ?it/s]Refuting Estimates:   1%|[32m1         [0m| 1/100 [00:00<00:10,  9.15it/s]Refuting Estimates:   2%|[32m2         [0m| 2/100 [00:00<00:10,  9.21it/s]Refuting Estimates:   3%|[32m3         [0m| 3/100 [00:00<00:10,  9.31it/s]Refuting Estimates:   4%|[32m4         [0m| 4/100 [00:00<00:10,  9.38it/s]Refuting Estimates:   5%|[32m5         [0m| 5/100 [00:00<00:10,  9.35it/s]Refuting Estimates:   6%|[32m6         [0m| 6/100 [00:00<00:10,  9.38it/s]Refuting Estimates:   7%|[32m7         [0m| 7/100 [00:00<00:09,  9.40it/s]Refuting Estimates:   8%|[32m8         [0m| 8/100 [00:00<00:10,  9.09it/s]Refuting Estimates:   9%|[32m9         [0m| 9/100 [00:00<00:10,  9.07it/s]Refuting Estimates:  10%|[32m#         [0m| 10/100 [00:01<00:09,  9.06it/s]Refuting Estimates:  11%|[32m#1        [0m| 11/100 [00:01<00:09,  8.99it/s]Refuting Estimates:  12%|[32m#2        [0m| 12/100 [00:01<00:09,  9.11it/s]Refuting Estimates:  13%|[32m#3        [0m| 13/100 [00:01<00:09,  9.06it/s]Refuting Estimates:  14%|[32m#4        [0m| 14/100 [00:01<00:09,  9.04it/s]Refuting Estimates:  15%|[32m#5        [0m| 15/100 [00:01<00:09,  9.07it/s]Refuting Estimates:  16%|[32m#6        [0m| 16/100 [00:01<00:09,  9.13it/s]Refuting Estimates:  17%|[32m#7        [0m| 17/100 [00:01<00:09,  9.16it/s]Refuting Estimates:  18%|[32m#8        [0m| 18/100 [00:01<00:09,  9.05it/s]Refuting Estimates:  19%|[32m#9        [0m| 19/100 [00:02<00:08,  9.01it/s]Refuting Estimates:  20%|[32m##        [0m| 20/100 [00:02<00:08,  9.11it/s]Refuting Estimates:  21%|[32m##1       [0m| 21/100 [00:02<00:08,  9.03it/s]Refuting Estimates:  22%|[32m##2       [0m| 22/100 [00:02<00:08,  9.01it/s]Refuting Estimates:  23%|[32m##3       [0m| 23/100 [00:02<00:08,  9.11it/s]Refuting Estimates:  24%|[32m##4       [0m| 24/100 [00:02<00:08,  9.16it/s]Refuting Estimates:  25%|[32m##5       [0m| 25/100 [00:02<00:08,  9.08it/s]Refuting Estimates:  26%|[32m##6       [0m| 26/100 [00:02<00:08,  9.07it/s]Refuting Estimates:  27%|[32m##7       [0m| 27/100 [00:02<00:08,  8.88it/s]Refuting Estimates:  28%|[32m##8       [0m| 28/100 [00:03<00:07,  9.03it/s]Refuting Estimates:  29%|[32m##9       [0m| 29/100 [00:03<00:07,  8.96it/s]Refuting Estimates:  30%|[32m###       [0m| 30/100 [00:03<00:07,  8.98it/s]Refuting Estimates:  31%|[32m###1      [0m| 31/100 [00:03<00:07,  9.08it/s]Refuting Estimates:  32%|[32m###2      [0m| 32/100 [00:03<00:07,  9.04it/s]Refuting Estimates:  33%|[32m###3      [0m| 33/100 [00:03<00:07,  9.12it/s]Refuting Estimates:  34%|[32m###4      [0m| 34/100 [00:03<00:07,  9.15it/s]Refuting Estimates:  35%|[32m###5      [0m| 35/100 [00:03<00:07,  9.25it/s]Refuting Estimates:  36%|[32m###6      [0m| 36/100 [00:03<00:06,  9.29it/s]Refuting Estimates:  37%|[32m###7      [0m| 37/100 [00:04<00:06,  9.20it/s]Refuting Estimates:  38%|[32m###8      [0m| 38/100 [00:04<00:06,  9.08it/s]Refuting Estimates:  39%|[32m###9      [0m| 39/100 [00:04<00:06,  9.05it/s]Refuting Estimates:  40%|[32m####      [0m| 40/100 [00:04<00:06,  8.93it/s]Refuting Estimates:  41%|[32m####1     [0m| 41/100 [00:04<00:06,  8.96it/s]Refuting Estimates:  42%|[32m####2     [0m| 42/100 [00:04<00:06,  9.07it/s]Refuting Estimates:  43%|[32m####3     [0m| 43/100 [00:04<00:06,  9.13it/s]Refuting Estimates:  44%|[32m####4     [0m| 44/100 [00:04<00:06,  9.16it/s]Refuting Estimates:  45%|[32m####5     [0m| 45/100 [00:04<00:06,  8.91it/s]Refuting Estimates:  46%|[32m####6     [0m| 46/100 [00:05<00:06,  8.94it/s]Refuting Estimates:  47%|[32m####6     [0m| 47/100 [00:05<00:05,  9.07it/s]Refuting Estimates:  48%|[32m####8     [0m| 48/100 [00:05<00:05,  9.09it/s]Refuting Estimates:  49%|[32m####9     [0m| 49/100 [00:05<00:05,  9.03it/s]Refuting Estimates:  50%|[32m#####     [0m| 50/100 [00:05<00:05,  9.06it/s]Refuting Estimates:  51%|[32m#####1    [0m| 51/100 [00:05<00:05,  9.12it/s]Refuting Estimates:  52%|[32m#####2    [0m| 52/100 [00:05<00:05,  9.11it/s]Refuting Estimates:  53%|[32m#####3    [0m| 53/100 [00:05<00:05,  9.07it/s]Refuting Estimates:  54%|[32m#####4    [0m| 54/100 [00:05<00:05,  8.76it/s]Refuting Estimates:  55%|[32m#####5    [0m| 55/100 [00:06<00:05,  8.62it/s]Refuting Estimates:  56%|[32m#####6    [0m| 56/100 [00:06<00:05,  8.62it/s]Refuting Estimates:  57%|[32m#####6    [0m| 57/100 [00:06<00:04,  8.75it/s]Refuting Estimates:  58%|[32m#####8    [0m| 58/100 [00:06<00:04,  8.77it/s]Refuting Estimates:  59%|[32m#####8    [0m| 59/100 [00:06<00:04,  8.89it/s]Refuting Estimates:  60%|[32m######    [0m| 60/100 [00:06<00:04,  8.98it/s]Refuting Estimates:  61%|[32m######1   [0m| 61/100 [00:06<00:04,  8.79it/s]Refuting Estimates:  62%|[32m######2   [0m| 62/100 [00:06<00:04,  8.96it/s]Refuting Estimates:  63%|[32m######3   [0m| 63/100 [00:06<00:04,  9.02it/s]Refuting Estimates:  64%|[32m######4   [0m| 64/100 [00:07<00:04,  8.96it/s]Refuting Estimates:  65%|[32m######5   [0m| 65/100 [00:07<00:03,  9.00it/s]Refuting Estimates:  66%|[32m######6   [0m| 66/100 [00:07<00:03,  8.76it/s]Refuting Estimates:  67%|[32m######7   [0m| 67/100 [00:07<00:03,  8.82it/s]Refuting Estimates:  68%|[32m######8   [0m| 68/100 [00:07<00:03,  8.99it/s]Refuting Estimates:  69%|[32m######9   [0m| 69/100 [00:07<00:03,  8.81it/s]Refuting Estimates:  70%|[32m#######   [0m| 70/100 [00:07<00:03,  8.92it/s]Refuting Estimates:  71%|[32m#######1  [0m| 71/100 [00:07<00:03,  8.96it/s]Refuting Estimates:  72%|[32m#######2  [0m| 72/100 [00:07<00:03,  8.96it/s]Refuting Estimates:  73%|[32m#######3  [0m| 73/100 [00:08<00:02,  9.02it/s]Refuting Estimates:  74%|[32m#######4  [0m| 74/100 [00:08<00:02,  8.85it/s]Refuting Estimates:  75%|[32m#######5  [0m| 75/100 [00:08<00:02,  8.97it/s]Refuting Estimates:  76%|[32m#######6  [0m| 76/100 [00:08<00:02,  8.99it/s]Refuting Estimates:  77%|[32m#######7  [0m| 77/100 [00:08<00:02,  8.90it/s]Refuting Estimates:  78%|[32m#######8  [0m| 78/100 [00:08<00:02,  9.01it/s]Refuting Estimates:  79%|[32m#######9  [0m| 79/100 [00:08<00:02,  8.95it/s]Refuting Estimates:  80%|[32m########  [0m| 80/100 [00:08<00:02,  9.00it/s]Refuting Estimates:  81%|[32m########1 [0m| 81/100 [00:08<00:02,  8.96it/s]Refuting Estimates:  82%|[32m########2 [0m| 82/100 [00:09<00:02,  8.90it/s]Refuting Estimates:  83%|[32m########2 [0m| 83/100 [00:09<00:01,  8.95it/s]Refuting Estimates:  84%|[32m########4 [0m| 84/100 [00:09<00:01,  8.87it/s]Refuting Estimates:  85%|[32m########5 [0m| 85/100 [00:09<00:01,  8.87it/s]Refuting Estimates:  86%|[32m########6 [0m| 86/100 [00:09<00:01,  8.95it/s]Refuting Estimates:  87%|[32m########7 [0m| 87/100 [00:09<00:01,  8.94it/s]Refuting Estimates:  88%|[32m########8 [0m| 88/100 [00:09<00:01,  8.71it/s]Refuting Estimates:  89%|[32m########9 [0m| 89/100 [00:09<00:01,  8.83it/s]Refuting Estimates:  90%|[32m######### [0m| 90/100 [00:10<00:01,  8.80it/s]Refuting Estimates:  91%|[32m#########1[0m| 91/100 [00:10<00:01,  8.83it/s]Refuting Estimates:  92%|[32m#########2[0m| 92/100 [00:10<00:00,  9.00it/s]Refuting Estimates:  93%|[32m#########3[0m| 93/100 [00:10<00:00,  9.01it/s]Refuting Estimates:  94%|[32m#########3[0m| 94/100 [00:10<00:00,  9.05it/s]Refuting Estimates:  95%|[32m#########5[0m| 95/100 [00:10<00:00,  8.98it/s]Refuting Estimates:  96%|[32m#########6[0m| 96/100 [00:10<00:00,  8.92it/s]Refuting Estimates:  97%|[32m#########7[0m| 97/100 [00:10<00:00,  8.99it/s]Refuting Estimates:  98%|[32m#########8[0m| 98/100 [00:10<00:00,  8.98it/s]Refuting Estimates:  99%|[32m#########9[0m| 99/100 [00:10<00:00,  9.06it/s]Refuting Estimates: 100%|[32m##########[0m| 100/100 [00:11<00:00,  9.08it/s]Refuting Estimates: 100%|[32m##########[0m| 100/100 [00:11<00:00,  9.00it/s]

print(res_random)
## Refute: Add a random common cause
## Estimated effect:11.250168750807648
## New effect:11.25016875080765
## p value:1.0
```

## 用安慰剂变量替换

将治疗变量随机打乱，破坏真实的因果效应；理论上估计效应应该接近于 0。

``` python
res_placebo = model.refute_estimate(
  identified_estimand, 
  estimate,
  method_name = "placebo_treatment_refuter",
  random_seed = 1,
  n_jobs = 1,
  show_progress_bar = True,
  placebo_type = "permute"
)
## Refuting Estimates:   0%|[32m          [0m| 0/100 [00:00<?, ?it/s]Refuting Estimates:   1%|[32m1         [0m| 1/100 [00:00<00:11,  8.85it/s]Refuting Estimates:   2%|[32m2         [0m| 2/100 [00:00<00:11,  8.78it/s]Refuting Estimates:   3%|[32m3         [0m| 3/100 [00:00<00:11,  8.73it/s]Refuting Estimates:   4%|[32m4         [0m| 4/100 [00:00<00:11,  8.67it/s]Refuting Estimates:   5%|[32m5         [0m| 5/100 [00:00<00:10,  8.73it/s]Refuting Estimates:   6%|[32m6         [0m| 6/100 [00:00<00:10,  8.76it/s]Refuting Estimates:   7%|[32m7         [0m| 7/100 [00:00<00:10,  8.77it/s]Refuting Estimates:   8%|[32m8         [0m| 8/100 [00:00<00:10,  8.79it/s]Refuting Estimates:   9%|[32m9         [0m| 9/100 [00:01<00:10,  8.89it/s]Refuting Estimates:  10%|[32m#         [0m| 10/100 [00:01<00:10,  8.93it/s]Refuting Estimates:  11%|[32m#1        [0m| 11/100 [00:01<00:10,  8.87it/s]Refuting Estimates:  12%|[32m#2        [0m| 12/100 [00:01<00:09,  8.86it/s]Refuting Estimates:  13%|[32m#3        [0m| 13/100 [00:01<00:09,  8.83it/s]Refuting Estimates:  14%|[32m#4        [0m| 14/100 [00:01<00:10,  8.43it/s]Refuting Estimates:  15%|[32m#5        [0m| 15/100 [00:01<00:10,  8.49it/s]Refuting Estimates:  16%|[32m#6        [0m| 16/100 [00:01<00:09,  8.64it/s]Refuting Estimates:  17%|[32m#7        [0m| 17/100 [00:01<00:09,  8.64it/s]Refuting Estimates:  18%|[32m#8        [0m| 18/100 [00:02<00:09,  8.57it/s]Refuting Estimates:  19%|[32m#9        [0m| 19/100 [00:02<00:09,  8.60it/s]Refuting Estimates:  20%|[32m##        [0m| 20/100 [00:02<00:09,  8.62it/s]Refuting Estimates:  21%|[32m##1       [0m| 21/100 [00:02<00:09,  8.68it/s]Refuting Estimates:  22%|[32m##2       [0m| 22/100 [00:02<00:09,  8.60it/s]Refuting Estimates:  23%|[32m##3       [0m| 23/100 [00:02<00:08,  8.70it/s]Refuting Estimates:  24%|[32m##4       [0m| 24/100 [00:02<00:08,  8.75it/s]Refuting Estimates:  25%|[32m##5       [0m| 25/100 [00:02<00:08,  8.75it/s]Refuting Estimates:  26%|[32m##6       [0m| 26/100 [00:02<00:08,  8.56it/s]Refuting Estimates:  27%|[32m##7       [0m| 27/100 [00:03<00:08,  8.39it/s]Refuting Estimates:  28%|[32m##8       [0m| 28/100 [00:03<00:08,  8.32it/s]Refuting Estimates:  29%|[32m##9       [0m| 29/100 [00:03<00:08,  8.32it/s]Refuting Estimates:  30%|[32m###       [0m| 30/100 [00:03<00:08,  8.50it/s]Refuting Estimates:  31%|[32m###1      [0m| 31/100 [00:03<00:08,  8.43it/s]Refuting Estimates:  32%|[32m###2      [0m| 32/100 [00:03<00:07,  8.58it/s]Refuting Estimates:  33%|[32m###3      [0m| 33/100 [00:03<00:07,  8.73it/s]Refuting Estimates:  34%|[32m###4      [0m| 34/100 [00:03<00:07,  8.63it/s]Refuting Estimates:  35%|[32m###5      [0m| 35/100 [00:04<00:07,  8.62it/s]Refuting Estimates:  36%|[32m###6      [0m| 36/100 [00:04<00:07,  8.74it/s]Refuting Estimates:  37%|[32m###7      [0m| 37/100 [00:04<00:07,  8.79it/s]Refuting Estimates:  38%|[32m###8      [0m| 38/100 [00:04<00:07,  8.81it/s]Refuting Estimates:  39%|[32m###9      [0m| 39/100 [00:04<00:06,  8.75it/s]Refuting Estimates:  40%|[32m####      [0m| 40/100 [00:04<00:06,  8.72it/s]Refuting Estimates:  41%|[32m####1     [0m| 41/100 [00:04<00:06,  8.83it/s]Refuting Estimates:  42%|[32m####2     [0m| 42/100 [00:04<00:06,  8.88it/s]Refuting Estimates:  43%|[32m####3     [0m| 43/100 [00:04<00:06,  8.98it/s]Refuting Estimates:  44%|[32m####4     [0m| 44/100 [00:05<00:06,  8.99it/s]Refuting Estimates:  45%|[32m####5     [0m| 45/100 [00:05<00:06,  8.64it/s]Refuting Estimates:  46%|[32m####6     [0m| 46/100 [00:05<00:06,  8.72it/s]Refuting Estimates:  47%|[32m####6     [0m| 47/100 [00:05<00:06,  8.68it/s]Refuting Estimates:  48%|[32m####8     [0m| 48/100 [00:05<00:05,  8.73it/s]Refuting Estimates:  49%|[32m####9     [0m| 49/100 [00:05<00:05,  8.71it/s]Refuting Estimates:  50%|[32m#####     [0m| 50/100 [00:05<00:05,  8.75it/s]Refuting Estimates:  51%|[32m#####1    [0m| 51/100 [00:05<00:05,  8.81it/s]Refuting Estimates:  52%|[32m#####2    [0m| 52/100 [00:05<00:05,  8.89it/s]Refuting Estimates:  53%|[32m#####3    [0m| 53/100 [00:06<00:05,  8.93it/s]Refuting Estimates:  54%|[32m#####4    [0m| 54/100 [00:06<00:05,  8.87it/s]Refuting Estimates:  55%|[32m#####5    [0m| 55/100 [00:06<00:05,  8.84it/s]Refuting Estimates:  56%|[32m#####6    [0m| 56/100 [00:06<00:05,  8.80it/s]Refuting Estimates:  57%|[32m#####6    [0m| 57/100 [00:06<00:04,  8.89it/s]Refuting Estimates:  58%|[32m#####8    [0m| 58/100 [00:06<00:04,  8.80it/s]Refuting Estimates:  59%|[32m#####8    [0m| 59/100 [00:06<00:04,  8.89it/s]Refuting Estimates:  60%|[32m######    [0m| 60/100 [00:06<00:04,  8.96it/s]Refuting Estimates:  61%|[32m######1   [0m| 61/100 [00:06<00:04,  9.05it/s]Refuting Estimates:  62%|[32m######2   [0m| 62/100 [00:07<00:04,  8.88it/s]Refuting Estimates:  63%|[32m######3   [0m| 63/100 [00:07<00:04,  8.70it/s]Refuting Estimates:  64%|[32m######4   [0m| 64/100 [00:07<00:04,  8.87it/s]Refuting Estimates:  65%|[32m######5   [0m| 65/100 [00:07<00:03,  8.95it/s]Refuting Estimates:  66%|[32m######6   [0m| 66/100 [00:07<00:03,  8.92it/s]Refuting Estimates:  67%|[32m######7   [0m| 67/100 [00:07<00:03,  8.80it/s]Refuting Estimates:  68%|[32m######8   [0m| 68/100 [00:07<00:03,  8.93it/s]Refuting Estimates:  69%|[32m######9   [0m| 69/100 [00:07<00:03,  9.01it/s]Refuting Estimates:  70%|[32m#######   [0m| 70/100 [00:07<00:03,  8.91it/s]Refuting Estimates:  71%|[32m#######1  [0m| 71/100 [00:08<00:03,  8.73it/s]Refuting Estimates:  72%|[32m#######2  [0m| 72/100 [00:08<00:03,  8.71it/s]Refuting Estimates:  73%|[32m#######3  [0m| 73/100 [00:08<00:03,  8.77it/s]Refuting Estimates:  74%|[32m#######4  [0m| 74/100 [00:08<00:02,  8.91it/s]Refuting Estimates:  75%|[32m#######5  [0m| 75/100 [00:08<00:02,  8.84it/s]Refuting Estimates:  76%|[32m#######6  [0m| 76/100 [00:08<00:02,  8.88it/s]Refuting Estimates:  77%|[32m#######7  [0m| 77/100 [00:08<00:02,  8.92it/s]Refuting Estimates:  78%|[32m#######8  [0m| 78/100 [00:08<00:02,  8.86it/s]Refuting Estimates:  79%|[32m#######9  [0m| 79/100 [00:09<00:02,  8.88it/s]Refuting Estimates:  80%|[32m########  [0m| 80/100 [00:09<00:02,  8.90it/s]Refuting Estimates:  81%|[32m########1 [0m| 81/100 [00:09<00:02,  8.90it/s]Refuting Estimates:  82%|[32m########2 [0m| 82/100 [00:09<00:02,  8.66it/s]Refuting Estimates:  83%|[32m########2 [0m| 83/100 [00:09<00:01,  8.76it/s]Refuting Estimates:  84%|[32m########4 [0m| 84/100 [00:09<00:01,  8.79it/s]Refuting Estimates:  85%|[32m########5 [0m| 85/100 [00:09<00:01,  8.92it/s]Refuting Estimates:  86%|[32m########6 [0m| 86/100 [00:09<00:01,  8.99it/s]Refuting Estimates:  87%|[32m########7 [0m| 87/100 [00:09<00:01,  9.04it/s]Refuting Estimates:  88%|[32m########8 [0m| 88/100 [00:10<00:01,  9.08it/s]Refuting Estimates:  89%|[32m########9 [0m| 89/100 [00:10<00:01,  8.85it/s]Refuting Estimates:  90%|[32m######### [0m| 90/100 [00:10<00:01,  8.95it/s]Refuting Estimates:  91%|[32m#########1[0m| 91/100 [00:10<00:00,  9.04it/s]Refuting Estimates:  92%|[32m#########2[0m| 92/100 [00:10<00:00,  8.89it/s]Refuting Estimates:  93%|[32m#########3[0m| 93/100 [00:10<00:00,  8.62it/s]Refuting Estimates:  94%|[32m#########3[0m| 94/100 [00:10<00:00,  8.42it/s]Refuting Estimates:  95%|[32m#########5[0m| 95/100 [00:10<00:00,  8.17it/s]Refuting Estimates:  96%|[32m#########6[0m| 96/100 [00:10<00:00,  8.32it/s]Refuting Estimates:  97%|[32m#########7[0m| 97/100 [00:11<00:00,  8.26it/s]Refuting Estimates:  98%|[32m#########8[0m| 98/100 [00:11<00:00,  8.39it/s]Refuting Estimates:  99%|[32m#########9[0m| 99/100 [00:11<00:00,  8.58it/s]Refuting Estimates: 100%|[32m##########[0m| 100/100 [00:11<00:00,  8.64it/s]Refuting Estimates: 100%|[32m##########[0m| 100/100 [00:11<00:00,  8.74it/s]

print(res_placebo)
## Refute: Use a Placebo Treatment
## Estimated effect:11.250168750807648
## New effect:0.0004172703150443136
## p value:0.98
```

## 移除数据中的随机子集

用部分数据重新估计，结果应该与原估计值相近。

``` python
res_subset = model.refute_estimate(
  identified_estimand,
  estimate,
  method_name = "data_subset_refuter",
  random_seed = 1,
  n_jobs = 1,
  show_progress_bar = True,
  subset_fraction = 0.9
)
## Refuting Estimates:   0%|[32m          [0m| 0/100 [00:00<?, ?it/s]Refuting Estimates:   1%|[32m1         [0m| 1/100 [00:00<00:10,  9.58it/s]Refuting Estimates:   3%|[32m3         [0m| 3/100 [00:00<00:10,  9.68it/s]Refuting Estimates:   4%|[32m4         [0m| 4/100 [00:00<00:10,  9.44it/s]Refuting Estimates:   5%|[32m5         [0m| 5/100 [00:00<00:10,  8.99it/s]Refuting Estimates:   6%|[32m6         [0m| 6/100 [00:00<00:10,  9.07it/s]Refuting Estimates:   7%|[32m7         [0m| 7/100 [00:00<00:10,  9.15it/s]Refuting Estimates:   8%|[32m8         [0m| 8/100 [00:00<00:10,  9.13it/s]Refuting Estimates:   9%|[32m9         [0m| 9/100 [00:00<00:09,  9.35it/s]Refuting Estimates:  11%|[32m#1        [0m| 11/100 [00:01<00:09,  9.72it/s]Refuting Estimates:  12%|[32m#2        [0m| 12/100 [00:01<00:09,  9.71it/s]Refuting Estimates:  13%|[32m#3        [0m| 13/100 [00:01<00:08,  9.78it/s]Refuting Estimates:  14%|[32m#4        [0m| 14/100 [00:01<00:08,  9.68it/s]Refuting Estimates:  15%|[32m#5        [0m| 15/100 [00:01<00:08,  9.48it/s]Refuting Estimates:  16%|[32m#6        [0m| 16/100 [00:01<00:08,  9.49it/s]Refuting Estimates:  18%|[32m#8        [0m| 18/100 [00:01<00:08,  9.64it/s]Refuting Estimates:  19%|[32m#9        [0m| 19/100 [00:02<00:08,  9.45it/s]Refuting Estimates:  20%|[32m##        [0m| 20/100 [00:02<00:08,  9.32it/s]Refuting Estimates:  21%|[32m##1       [0m| 21/100 [00:02<00:08,  9.37it/s]Refuting Estimates:  22%|[32m##2       [0m| 22/100 [00:02<00:08,  9.48it/s]Refuting Estimates:  23%|[32m##3       [0m| 23/100 [00:02<00:08,  9.22it/s]Refuting Estimates:  24%|[32m##4       [0m| 24/100 [00:02<00:08,  9.16it/s]Refuting Estimates:  25%|[32m##5       [0m| 25/100 [00:02<00:08,  9.34it/s]Refuting Estimates:  26%|[32m##6       [0m| 26/100 [00:02<00:07,  9.43it/s]Refuting Estimates:  28%|[32m##8       [0m| 28/100 [00:02<00:07,  9.73it/s]Refuting Estimates:  29%|[32m##9       [0m| 29/100 [00:03<00:07,  9.70it/s]Refuting Estimates:  31%|[32m###1      [0m| 31/100 [00:03<00:07,  9.86it/s]Refuting Estimates:  32%|[32m###2      [0m| 32/100 [00:03<00:06,  9.85it/s]Refuting Estimates:  33%|[32m###3      [0m| 33/100 [00:03<00:06,  9.79it/s]Refuting Estimates:  34%|[32m###4      [0m| 34/100 [00:03<00:06,  9.69it/s]Refuting Estimates:  35%|[32m###5      [0m| 35/100 [00:03<00:06,  9.48it/s]Refuting Estimates:  36%|[32m###6      [0m| 36/100 [00:03<00:06,  9.50it/s]Refuting Estimates:  38%|[32m###8      [0m| 38/100 [00:03<00:06,  9.51it/s]Refuting Estimates:  39%|[32m###9      [0m| 39/100 [00:04<00:06,  9.47it/s]Refuting Estimates:  40%|[32m####      [0m| 40/100 [00:04<00:06,  9.60it/s]Refuting Estimates:  41%|[32m####1     [0m| 41/100 [00:04<00:06,  9.57it/s]Refuting Estimates:  42%|[32m####2     [0m| 42/100 [00:04<00:06,  9.56it/s]Refuting Estimates:  43%|[32m####3     [0m| 43/100 [00:04<00:05,  9.68it/s]Refuting Estimates:  45%|[32m####5     [0m| 45/100 [00:04<00:05,  9.79it/s]Refuting Estimates:  46%|[32m####6     [0m| 46/100 [00:04<00:05,  9.84it/s]Refuting Estimates:  47%|[32m####6     [0m| 47/100 [00:04<00:05,  9.81it/s]Refuting Estimates:  48%|[32m####8     [0m| 48/100 [00:05<00:05,  9.68it/s]Refuting Estimates:  49%|[32m####9     [0m| 49/100 [00:05<00:05,  9.73it/s]Refuting Estimates:  50%|[32m#####     [0m| 50/100 [00:05<00:05,  9.76it/s]Refuting Estimates:  51%|[32m#####1    [0m| 51/100 [00:05<00:05,  9.73it/s]Refuting Estimates:  52%|[32m#####2    [0m| 52/100 [00:05<00:05,  9.51it/s]Refuting Estimates:  53%|[32m#####3    [0m| 53/100 [00:05<00:05,  9.33it/s]Refuting Estimates:  54%|[32m#####4    [0m| 54/100 [00:05<00:04,  9.50it/s]Refuting Estimates:  55%|[32m#####5    [0m| 55/100 [00:05<00:04,  9.53it/s]Refuting Estimates:  56%|[32m#####6    [0m| 56/100 [00:05<00:04,  9.37it/s]Refuting Estimates:  57%|[32m#####6    [0m| 57/100 [00:05<00:04,  9.49it/s]Refuting Estimates:  58%|[32m#####8    [0m| 58/100 [00:06<00:04,  9.61it/s]Refuting Estimates:  59%|[32m#####8    [0m| 59/100 [00:06<00:04,  9.66it/s]Refuting Estimates:  60%|[32m######    [0m| 60/100 [00:06<00:04,  9.68it/s]Refuting Estimates:  62%|[32m######2   [0m| 62/100 [00:06<00:03,  9.75it/s]Refuting Estimates:  64%|[32m######4   [0m| 64/100 [00:06<00:03,  9.92it/s]Refuting Estimates:  65%|[32m######5   [0m| 65/100 [00:06<00:03,  9.76it/s]Refuting Estimates:  66%|[32m######6   [0m| 66/100 [00:06<00:03,  9.77it/s]Refuting Estimates:  67%|[32m######7   [0m| 67/100 [00:06<00:03,  9.71it/s]Refuting Estimates:  69%|[32m######9   [0m| 69/100 [00:07<00:03,  9.93it/s]Refuting Estimates:  70%|[32m#######   [0m| 70/100 [00:07<00:03,  9.93it/s]Refuting Estimates:  71%|[32m#######1  [0m| 71/100 [00:07<00:02,  9.89it/s]Refuting Estimates:  72%|[32m#######2  [0m| 72/100 [00:07<00:02,  9.79it/s]Refuting Estimates:  74%|[32m#######4  [0m| 74/100 [00:07<00:02,  9.72it/s]Refuting Estimates:  75%|[32m#######5  [0m| 75/100 [00:07<00:02,  9.76it/s]Refuting Estimates:  76%|[32m#######6  [0m| 76/100 [00:07<00:02,  9.67it/s]Refuting Estimates:  77%|[32m#######7  [0m| 77/100 [00:08<00:02,  9.63it/s]Refuting Estimates:  79%|[32m#######9  [0m| 79/100 [00:08<00:02,  9.80it/s]Refuting Estimates:  81%|[32m########1 [0m| 81/100 [00:08<00:01,  9.84it/s]Refuting Estimates:  82%|[32m########2 [0m| 82/100 [00:08<00:01,  9.77it/s]Refuting Estimates:  83%|[32m########2 [0m| 83/100 [00:08<00:01,  9.24it/s]Refuting Estimates:  84%|[32m########4 [0m| 84/100 [00:08<00:01,  8.96it/s]Refuting Estimates:  85%|[32m########5 [0m| 85/100 [00:09<00:02,  6.72it/s]Refuting Estimates:  86%|[32m########6 [0m| 86/100 [00:09<00:02,  5.61it/s]Refuting Estimates:  87%|[32m########7 [0m| 87/100 [00:09<00:02,  5.92it/s]Refuting Estimates:  88%|[32m########8 [0m| 88/100 [00:09<00:01,  6.40it/s]Refuting Estimates:  89%|[32m########9 [0m| 89/100 [00:09<00:01,  6.23it/s]Refuting Estimates:  90%|[32m######### [0m| 90/100 [00:09<00:01,  6.53it/s]Refuting Estimates:  91%|[32m#########1[0m| 91/100 [00:09<00:01,  7.08it/s]Refuting Estimates:  92%|[32m#########2[0m| 92/100 [00:10<00:01,  7.54it/s]Refuting Estimates:  93%|[32m#########3[0m| 93/100 [00:10<00:00,  7.76it/s]Refuting Estimates:  94%|[32m#########3[0m| 94/100 [00:10<00:00,  8.18it/s]Refuting Estimates:  95%|[32m#########5[0m| 95/100 [00:10<00:00,  8.65it/s]Refuting Estimates:  96%|[32m#########6[0m| 96/100 [00:10<00:00,  8.88it/s]Refuting Estimates:  98%|[32m#########8[0m| 98/100 [00:10<00:00,  9.30it/s]Refuting Estimates:  99%|[32m#########9[0m| 99/100 [00:10<00:00,  9.37it/s]Refuting Estimates: 100%|[32m##########[0m| 100/100 [00:10<00:00,  9.45it/s]Refuting Estimates: 100%|[32m##########[0m| 100/100 [00:10<00:00,  9.15it/s]
        
print(res_subset)
## Refute: Use a subset of data
## Estimated effect:11.250168750807648
## New effect:11.312134931369112
## p value:0.5
```

## 添加未观测共同原因变量

这种验证方法不是通过 p
值来判断结果，而是作为一种敏感性分析，用于衡量如果**所有共同原因都已观测**这一假设不成立时，估计结果会多快发生改变。具体来说，它检验对**后门假设**违反的敏感性：即假设所有的共同原因都被观测到。

也可以将该方法理解为，假设原始数据中本就存在未观测混杂，改变处理和结果的值，消除原本未观测共同原因的影响，然后再重新估计因果效应。理想情况下，原始和新估计值的差异应在一个可接受的范围内。

模拟存在未观测混杂因素的情况，测试估计值对混杂的敏感程度。

### 单一强度下的敏感性分析

``` python
res_unobserved = model.refute_estimate(
  identified_estimand,
  estimate, 
  random_seed = 1,
  n_jobs = 1,
  method_name = "add_unobserved_common_cause",
  confounders_effect_on_treatment = "binary_flip",
  confounders_effect_on_outcome = "linear",
  effect_strength_on_treatment = 0.01,
  effect_strength_on_outcome = 0.02
)

print(res_unobserved)
## Refute: Add an Unobserved Common Cause
## Estimated effect:11.250168750807648
## New effect:11.184766098500258
```

假设我们设定未观测共同原因 U 对处理的效应为 0.01，对结果的效应为
0.02。也可以多次尝试不同强度，绘制敏感性曲线，直观展示估计随未观测混杂效应变化的趋势。

通常，更有用的做法是观察未观测混杂效应逐步增强时，估计因果效应的变化趋势，可以向验证方法提供一组假定的混杂变量效应。方法会针对每一组假定值分别进行敏感性分析，输出在这些不同未观测混杂影响下，估计因果效应的最小值和最大值（即区间范围）。

``` python
res_unobserved_range = model.refute_estimate(
  identified_estimand, estimate,
  random_seed = 1,
  n_jobs = 1,
  method_name = "add_unobserved_common_cause",
  confounders_effect_on_treatment = "binary_flip",
  confounders_effect_on_outcome = "linear",
  effect_strength_on_treatment = np.array([0.001, 0.005, 0.01, 0.02]), 
  effect_strength_on_outcome = 0.01
)
```

<img src="计算因果效应的基本示例_files/figure-markdown_strict/unnamed-chunk-12-5.png" width="576" />

``` python

print(res_unobserved_range)
## Refute: Add an Unobserved Common Cause
## Estimated effect:11.250168750807648
## New effect:(10.476642918519385, 11.215118666125967)
```

上面的图展示了随着假定混杂对处理的影响增强，估计的因果效应逐渐减小。

结合领域知识，我们通常能够判断混杂对处理变量的最大合理影响范围。只要在这个范围内，因果效应的估计值始终大于零，我们就可以较为有信心地认为处理变量
v0 的因果效应为正。

### 二维热力图表示

``` python
res_unobserved_range = model.refute_estimate(
  identified_estimand, 
  estimate,
  random_seed = 1,
  n_jobs = 1,
  method_name = "add_unobserved_common_cause",
  confounders_effect_on_treatment = "binary_flip",
  confounders_effect_on_outcome = "linear",
  effect_strength_on_treatment = [0.001, 0.005, 0.01, 0.02],
  effect_strength_on_outcome = [0.001, 0.005, 0.01,0.02]
)
```

<img src="计算因果效应的基本示例_files/figure-markdown_strict/unnamed-chunk-13-7.png" width="576" />

``` python

print(res_unobserved_range)
## Refute: Add an Unobserved Common Cause
## Estimated effect:11.250168750807648
## New effect:(7.946540748041179, 11.164309815619722)
```

### 自动推断混杂影响强度参数

DoWhy
支持自动选择未观测混杂变量的效应强度参数。其核心假设是，未观测混杂变量对处理或结果的效应不会强于任何已观测混杂变量。也就是说，我们已经收集到了最相关的混杂变量数据。基于这个假设，DoWhy
会自动使用已观测混杂中最大效应强度来作为未观测混杂影响的上界，从而自动设置
effect_strength_on_treatment 和 effect_strength_on_outcome
的取值范围。

``` python
res_unobserved_auto = model.refute_estimate(
  identified_estimand, 
  estimate,
  random_seed = 1,
  n_jobs = 1,
  method_name = "add_unobserved_common_cause",
  confounders_effect_on_treatment = "binary_flip",
  confounders_effect_on_outcome = "linear"
)
```

<img src="计算因果效应的基本示例_files/figure-markdown_strict/unnamed-chunk-14-9.png" width="576" />

``` python

print(res_unobserved_auto)
## Refute: Add an Unobserved Common Cause
## Estimated effect:11.250168750807648
## New effect:(-0.5424084016204305, 10.91905973887223)
```

# 结论

假设未观测混杂变量对处理或结果的影响不强于任何已观测混杂变量，则可以得出结论：处理的因果效应为正。

换句话说，在这一合理的敏感性假设下，即便考虑了潜在的未观测混杂，估计出的因果效应依然是正向的，因此我们的结论具有较强的稳健性。
